{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Continuous Data\n",
    "\n",
    "We have previously done some work with continuous data when we were learning about visualizations. In this section we are going to look at some methods that will help you examine associations between continuous variables. The primary tool we are going to use is the correlation. \n",
    "\n",
    "In this section, we will use a new library for visualizations, called `seaborn`. Seaborn is traditionally imported with the abbreviation `sns`\n",
    "\n",
    "The specific methods we will use that we haven't covered before are:\n",
    "\n",
    "* `corr()`, a dataframe method from pandas\n",
    "* `heatmap()`, a plotting method from seaborn\n",
    "* `pairplot()`, a plotting method from seaborn\n",
    "* `unique()` a series method from pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For slightly nicer charts\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Preparing our Dataset\n",
    "\n",
    "For this section we will use a selection from the dataset `fandango_score_comparison.csv` ([source](https://github.com/fivethirtyeight/data/tree/master/fandango)), containing data about movie ratings from Fandango, Rotten Tomatoes, IMDB, and Metacritic. The dataset was used for this article, [Be Suspicious Of Online Movie Ratings, Especially Fandangoâ€™s](https://fivethirtyeight.com/features/fandango-movies-ratings/), published by [fivethirtyeight](https://fivethirtyeight.com/).\n",
    "\n",
    "First, we read a CSV file of the movie ratings and assign it to a variable called `df_full`. We will then select a subset of the data and assign the subsection to the variable `df`.\n",
    "Each row in the dataset represents one movie.  The columns are labelled as follows: \n",
    "\n",
    "* FILM: The film in question\n",
    "* RottenTomatoes: The Rotten Tomatoes Tomatometer score for the film\n",
    "* RottenTomatoes_User: The Rotten Tomatoes user score for the film\n",
    "* Metacritic: The Metacritic critic score for the film\n",
    "* Metacritic_User: The Metacritic user score for the film\n",
    "* IMDB: The IMDb user score for the film\n",
    "* Fandango_Stars: The number of stars the film had on its Fandango movie page\n",
    "* Fandango_Ratingvalue: The Fandango ratingValue for the film, as pulled from the HTML of each page. This is the actual average score the movie obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"fandango_score_comparison.csv\")\n",
    "df = df_full.loc[:,'FILM':'Fandango_Ratingvalue'] # this creates a subset of the dataframe using location based indexing, which we will discuss later \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 146 movies (one per row) and eight columns. The first column labelled `'FILM'` contains strings with the name of the Film and its year of release. The remaining columns are either integers or floats representing various kinds of ratings of the quality of the film.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start looking at how the ratings are related to one another, let's look at some descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the variations in the mean, min, and max values across the different ratings sites. Some of the ratings systems appear to be using 100 points scales, others 10 points scales, and others 5 points scales. Also, no matter what the scale, the average rating tends to be just a bit higher than the mid-point of the scale. \n",
    "\n",
    "The different scales make looking at the descriptive statistics a bit confusing so let's convert all the scales to be the same. This conversion process is commonly called 'normalizing' your data. In this case we are going to convert everything to a 10-point scale. \n",
    "\n",
    "We are NOT going to 'copy over' the existing values. Instead we are going to calculate new values and assign them to new columns.  \n",
    "\n",
    "First, we will normalize the 100-point columns and assign them to new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_list = ['RottenTomatoes', 'RottenTomatoes_User', 'Metacritic']\n",
    "df[['RottenTomatoes_Norm', 'RottenTomatoes_User_Norm', 'Metacritic_Norm']] = df[hundred_list]/10\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will normalize the 5-point columns and assign them to new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_list = ['Fandango_Stars', 'Fandango_Ratingvalue']\n",
    "df[['Fandango_Stars_Norm', 'Fandango_Ratingvalue_Norm']] = df[five_list]*2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to drop all of the non-normed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_normed_ratings_list = ['RottenTomatoes', 'RottenTomatoes_User', 'Metacritic', 'Fandango_Stars', 'Fandango_Ratingvalue']\n",
    "df = df.drop(columns=non_normed_ratings_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Small Aside About Reusable Code\n",
    "\n",
    "So our output above looks great but our column order is bit jumbled. Let's do a quick fix to get the ratings in alphabetical order. We are going to need a list with the labels in the proper order first. We can make a list manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_order_list_manual = ['FILM', 'Fandango_Ratingvalue_Norm', 'Fandango_Stars_Norm', 'IMDB', 'Metacritic_Norm', 'Metacritic_User', 'RottenTomatoes_Norm', 'RottenTomatoes_User_Norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can make a list using a few properties and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_order_list_fancy = df.columns.tolist()\n",
    "label_order_list_fancy.sort() #Note that the sort() method sorts the list 'in place' and does not return anything, therefore it cannot be assigned to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods produce the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Manual result:', label_order_list_manual)\n",
    "print('Fancy  result:', label_order_list_fancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small datasets, manually creating lists may make sense and may be an efficient use of your time. However, it will often be the case that writing code to automate these types of processes will save you time and make your process less error prone. This will be particularly true for large datasets. Another advatage of solving your problems with code is the code can often be reused. The code for the 'fancy' approach above could be used with any dataset, while the manually created list will likely only ever be useful this one specific time. \n",
    "\n",
    "So we now have our list (we can use either), let's use it to 'reorder' our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[label_order_list_fancy]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good. Let's rerun our descriptives and take another look at the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 'normed' the data the difference in mean ratings across the rating sites really stands out. (For more about this issue see [the original article](https://fivethirtyeight.com/features/fandango-movies-ratings/)). However, what we are interested in right now is the degree to which the various ratings are associated (or how much they covary). \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Associations Using Correlations\n",
    "We often want to know the degree to which one variable is associated with another. We don't always say 'associated' when asking these types of questions; instead, we will often use the word 'predict'. For example, we might say we want to know if the price of milk will 'predict' the price of cheese. Unfortunately, the word 'predict' (or the phrase 'this predicts that') often implies things we (or our data) are not prepared to address. So it's a bit more cautious to talk about whether or not the price of milk is 'associated' with the price of cheese.\n",
    "\n",
    "If you are interested in how two variable are associated there are a bunch of formal statistical tests you might use to characterize the association. The type of test you would use depends on the type of data you are trying to associate. The most common case involves characterizing the linear association between two continuous variables. The test we are going to use to do this is formally called [Pearson's correlation coefficient, Pearson's r, or a bivariate correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). You might be tempted just to call it a correlation, and people often do this; but there are other types of correlational tests that are more appropriate for other types of data so its better to be specific about what type of correlational test you are using. \n",
    "\n",
    "Pearson's r has values between -1 and +1, with -1 denoting a perfect negative association (as one variable increases the other decreases). +1 denotes a perfect positive association (as one variable increase the other increases to the same degree). 0 denotes no association at all between the two variables.\n",
    "\n",
    "There is a built in method for looking at correlations in Pandas. If we index a single column from the dataframe, and then pass a second column as an argument to the `.corr()` method it will return a single float value. By default, `.corr()` returns Pearson's r; however, you could use a parameter to specify other types of correlations. \n",
    "\n",
    "Let's look at the association between movie ratings from Metacritic users and from RottenTomatoes_Users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Metacritic_User'].corr(df['RottenTomatoes_User_Norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating are positively correlated, and the association is fairly strong but isn't perfect. Now would be a good time for a scatterplot to get a better sense of what this r value means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x='Metacritic_User', y='RottenTomatoes_User_Norm', data=df)\n",
    "plt.xlabel(\"Metacritic Rating\")\n",
    "plt.ylabel(\"RT Rating\")\n",
    "plt.title(\"Metacritic vs RT Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So an r-value of .68 means that we generally see that movies scoring high on one site are also scoring high on the other site. BUT, there are a some exceptions. A few movies that are rated below a five on metacritic but above a seven on Rotten Tomatoes. \n",
    "\n",
    "Just for fun, let's check into that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weird = df.query(\" (Metacritic_User < 5) and (RottenTomatoes_User_Norm > 7) \")\n",
    "df_weird.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's some strange data. See if you can come up with an idea for why these films ratings would be so inconsistent. Poking around in df_full might help you out here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the scatter plot above... It looks like there might be a stronger association between the two ratings for movies that are highly rated and less of an association for movies that are not as highly rated. There are some formal statistical tests for this type of situation, but for now let's just look at it with the tools we have. \n",
    "\n",
    "Let's select only the movies that are rated less than 5 by metacritic and then rerun our correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta5_low = df.query('Metacritic_User < 5 ')\n",
    "df_meta5_low['Metacritic_User'].corr(df_meta5_low['RottenTomatoes_User_Norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be less agreement. Just to make a more apples to apple comparison, we can rerun this looking at films scoring greater than 5 on Metacritic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta5_high = df.query(\" Metacritic_User > 5 \")\n",
    "df_meta5_high['Metacritic_User'].corr(df_meta5_high['RottenTomatoes_User_Norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its notable that while there is a stronger assocation between ratings at the high end of the scale, using the full range of movies actually showed the strongest association between the ratings. \n",
    "***\n",
    "\n",
    "\n",
    "If we call the corr() method on the entire dataframe, pandas will calculate correlations for all columns with numeric datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is often referred to as a correlation table. This correlation table allows you to see how all of our movie rating variables are correlated with one another. \n",
    "Notice there is a diagonal of 1.000 values in this output. This is the variables correlated with themselves, which always results perfect positive correlations. It is also worth noting that the left and right sides of that diagonal are redundant mirror-images of one another. Since both the diagonal values and the mirror image values provide no real information, you will sometimes see these values replaced with some other kind of statistic that provides other information. We will see an example of this in just a minute. \n",
    "\n",
    "Giant tables of correlation values are often challenging to interpret on thier own. One way to help is to generate a visualization of the table itself. We can do this using something called a heatmap. \n",
    "In a heat map colors are used to represent different values.\n",
    "\n",
    "To make a heatmap we are going to import a new library called `seaborn`. You can think of `seaborn` as an extension of `matplotlib`. It does similar things in similar ways, but also makes some things easier to do. We are going to call the heatmap method from the seaborn library and we are going to pass it the correlation table above. We are also going to include the argument `cmap='Blues'` to control the type of colors that are displayed in the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_table = df.corr()\n",
    "sns.heatmap(correlation_table, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for visualizing associations across a data set is a scatter matrix. A scatter matrix is a set of scatterplots that visualize the information you would find in a correlation table.\n",
    "We are going to use the seaborn `pairplot()` method and pass it our dataframe to create our scatter matrix of movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df) # this code creates the scatter matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of information packed into the scatter matrix. First, notice the diagonals. Rather than showing us a perfect (and uninformative) correlation this scatter matrix if showing us histograms for each variable. From the histograms we wan see that 'fandango_starred_norm' only has discreet integer values. The histograms also show us that the Fandango ratings and the Rotten Tomato ratings tend to be skewed toward the positive side of the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Looking at Associations by Group (Category)\n",
    "We will sometimes want to know if associations vary by group. For example, if our movie rating dataset had movie genre data we might be interested in finding out if the ratings from different sites more strongly associated for dramas than for comedies. Or if we had multiple years of data we might be interested to see if the associations changed by year. \n",
    "\n",
    "With some slight modifications to the tools we have already discussed we can start looking at associations by group. To do this we are going to use a new dataset, `'recent-grads.csv'`, which contains data about employment and salaries for recent college graduates. The data comes from [here](https://github.com/fivethirtyeight/data/tree/master/college-majors) and was used for the story [The Economic Guide to Picking Your Major](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/), published by [fivethirtyeight](https://fivethirtyeight.com/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad_full = pd.read_csv(\"recent-grads.csv\") # here we read the full dataset\n",
    "df_grad = df_grad_full[['Major_category', 'Major', 'ShareWomen', 'Unemployment_rate','Median']] # here we use label based indexing to create a dataset that only contains a few columns of interest\n",
    "df_grad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 173 rows in the dataframe. Each row represents a different major. The majors are grouped into one of 16 categories. We can see the categories by calling the `unique()` method on the `'Major_category'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad['Major_category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining categories represent the following: \n",
    "* ShareWomen: Percentage of graduates classified as women\n",
    "* Unemployment_rate: Rate of unemployment\n",
    "* Median: Median earnings of full-time, year-round workers\n",
    "\n",
    "If we use `corr()` to look at our dataframe, one result immediately stands out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a scatter matrix to further examine this association. This time we are only going to look at associations for two different categories of majors.. First, we will use query to make a dataset that only contains categories that we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad_sub = df_grad.query(' Major_category == \"Engineering\" or Major_category == \"Psychology & Social Work\" or Major_category == \"Business\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the seaborn `pairplot()` method again, but with one small addition. We will to pass the argument `'Major_category'` to the parameter `hue`. This parameter produces pairplots with groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_grad_sub, hue='Major_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonals are again providing information about the frequency of specific values, but in this case the frequencies are represented in different colors for the different categories of majors. The same is done for the scatterplots. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
